server:
  port: 8420
  host: "0.0.0.0"

database:
  url: "postgres://contextify:contextify_local@localhost:5432/contextify?sslmode=disable"
  max_open_conns: 25
  max_idle_conns: 5
  conn_max_lifetime: 5m

embedding:
  provider: ollama
  ollama_url: "http://localhost:11434"
  model: "nomic-embed-text"
  dimensions: 768

memory:
  default_ttl: 86400        # 24 hours in seconds
  promote_access_count: 5   # auto-promote after N accesses
  promote_importance: 0.8   # auto-promote above this importance
  ttl_extend_factor: 0.5    # extend TTL by this factor on access
  cleanup_interval: 5m      # expired memory cleanup interval

search:
  vector_weight: 0.7        # weight for vector similarity in hybrid search
  keyword_weight: 0.3       # weight for keyword match in hybrid search
  default_limit: 20
  max_limit: 100
  cache_enabled: true       # enable hot-query search cache
  cache_ttl: 30s            # cache item TTL
  cache_max_entries: 500    # max number of cached query keys

steward:
  enabled: false            # safe default: off until explicitly enabled
  dry_run: true             # if enabled, dry-run by default
  tick_interval: 30s        # worker tick interval
  claim_batch_size: 10
  max_attempts: 3
  request_timeout: 30s
  model: "qwen2.5:3b"
  ollama_url: ""            # optional override; falls back to embedding.ollama_url
  auto_merge_threshold: 0.92
  auto_merge_from_suggestions: true
  llm_conflict_guard_enabled: false

  derivation:
    enabled: false
    max_candidates: 3
    min_confidence: 0.80
    min_novelty: 0.20

  self_learn:
    enabled: false
    eval_interval: 24h
    min_sample_size: 100

  retention:
    run_log_days: 14
    event_log_days: 14
